{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOd4ni6cV0ZepVF+gGDuwIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjcremer/NLP_Masters/blob/main/nlp243_assignment2_1542248.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubOI2KdgpvBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba14f2d3-631c-4c3d-9fab-41dd73cb7b74"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.functional import cross_entropy\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchtext.legacy\n",
        "#from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from collections import defaultdict\n",
        "#!pip install fasttext\n",
        "#import fasttext\n",
        "#import fasttext.util\n",
        "#fasttext.util.download_model('en', if_exists='ignore')\n",
        "#embedding_model = fasttext.load_model('cc.en.300.bin')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZLidThwBulm",
        "outputId": "645d6b93-afae-4880-9cac-20ff9382c035"
      },
      "source": [
        "!pip install poutyne\n",
        "from poutyne import set_seeds\n",
        "from poutyne.framework import Experiment"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.7/dist-packages (1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from poutyne) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from poutyne) (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->poutyne) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBegw_t_q3Qd"
      },
      "source": [
        "class JUtils():\n",
        "  def get_dist_df(list):\n",
        "    fdist = dict(nltk.FreqDist(list))\n",
        "    fdist = pd.DataFrame.from_dict(fdist, orient='index')\n",
        "    fdist.columns = ['Frequency']\n",
        "    fdist.index.name = 'Class'\n",
        "    fdist = fdist.sort_values(by=['Frequency'], ascending=False)\n",
        "    return fdist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipQivbT1Vb6b"
      },
      "source": [
        "class Preprocessor():\n",
        "  def __innit__(self):\n",
        "    self.lb = LabelBinarizer()\n",
        "\n",
        "  def get_data(self, filename='train_data.csv', vectorize=True, class_column='IOB Slot tags'):\n",
        "    self.class_column=class_column\n",
        "    tdf = pd.read_csv(filename)\n",
        "    if vectorize:\n",
        "      for i, row in tdf.iterrows():\n",
        "        tdf.at[i, class_column] = row[class_column].split()\n",
        "    self.raw_train_data = tdf\n",
        "    return self.raw_train_data\n",
        "\n",
        "  def split_data(self, df, split=0.1):\n",
        "    dev_set = df.sample(frac = split)\n",
        "    dev_set.reset_index(inplace=True, drop=True)\n",
        "    train_set = df.drop(dev_set.index)\n",
        "    train_set.reset_index(inplace=True, drop=True)\n",
        "    return train_set, dev_set\n",
        "\n",
        "  def format_IOB_data(self, df):\n",
        "    self.class_fdist = dict()\n",
        "    self.class_merge = dict()\n",
        "    for tags in df[self.class_column]:\n",
        "      for tag in tags:\n",
        "        merged_class = re.match('(I|B)_(\\w+)', tag)\n",
        "        self.class_merge[tag] = (merged_class != None and merged_class.group(2)) or tag\n",
        "    df[self.class_column] = df[self.class_column].apply(self.__reduce_classes__)\n",
        "    return df\n",
        "\n",
        "  def __reduce_classes__(self, classes):\n",
        "    new_classes = []\n",
        "    for i in classes:\n",
        "      new_classes.append(self.class_merge[i])\n",
        "      self.class_fdist[self.class_merge[i]] = self.class_fdist.get(self.class_merge[i], 0) + 1\n",
        "    return new_classes\n",
        "  \n",
        "  def __remove_classes_below_threshold__(self, df, threshold=5):\n",
        "    self.class_merge = dict()\n",
        "    new_class = 'other'\n",
        "    for original_class, row in self.original_class_fdist.iterrows():\n",
        "      if (row[0] > threshold):\n",
        "        self.class_merge[original_class] = original_class\n",
        "      else:\n",
        "        self.class_merge[original_class] = new_class\n",
        "    df[self.class_column] = df[self.class_column].apply(lambda x: self.class_merge[x])\n",
        "    return df\n",
        "\n",
        "  def _tokenize_utterance(self, utterance):\n",
        "    return word_tokenize(utterance)\n",
        "\n",
        "  def classify_relation(self, df):\n",
        "    self.relation_to_class = dict()\n",
        "    index = 0\n",
        "    for relation in df[self.class_column]:\n",
        "      if relation not in self.relation_to_class:\n",
        "        self.relation_to_class[relation] = index\n",
        "        index+=1\n",
        "    df[self.class_column] = df[self.class_column].apply(lambda x: self.relation_to_class[x])\n",
        "    return df\n",
        "\n",
        "  def tokenize_label(self, str):\n",
        "    return self.relation_to_class[str]\n",
        "\n",
        "  def format_relations_data(self, df):\n",
        "      self.original_class_fdist = JUtils.get_dist_df(df[self.class_column])\n",
        "      df = self.__remove_classes_below_threshold__(df)\n",
        "      df['utterances'] = df['utterances'].apply(lambda x: self._tokenize_utterance(x))\n",
        "      return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYsSMM_LRH85"
      },
      "source": [
        "class Vectorizer:\n",
        "  def __init__(self, embedding_model):\n",
        "    self.embedding_model = embedding_model\n",
        "  \n",
        "  def __call__(self, utterance):\n",
        "    embeddings = []\n",
        "    for token in utterance:\n",
        "      embeddings.append(self.embedding_model[token])\n",
        "    return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrkLh7J7Rwv_"
      },
      "source": [
        "class TagDataset(Dataset):\n",
        "  def __init__(self, path):\n",
        "    pp = Preprocessor()\n",
        "    data = pp.get_data(filename=path, vectorize=False, class_column='Core Relations')\n",
        "    #new_data = pp.format_relations_data(data)\n",
        "    self.data = pp.classify_relation(data)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data.index)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    data = self.data.iloc[index]\n",
        "    return {'text': data['utterances'], 'label': data['Core Relations']}\n",
        "\n",
        "#bla = TagDataset('train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh4925h2BIAS"
      },
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "pp = Preprocessor()\n",
        "data = pp.get_data(filename='train_data_merged_labels.csv', vectorize=False, class_column='Core Relations')\n",
        "data = pp.classify_relation(data)\n",
        "train_data, dev_data = pp.split_data(data)\n",
        "train_data.to_csv('train.csv', index=False)\n",
        "dev_data.to_csv('valid.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4POP0EWMViY"
      },
      "source": [
        "train = TagDataset('train.csv')\n",
        "valid = TagDataset('valid.csv')\n",
        "\n",
        "train_dataloader = DataLoader(train, batch_size=32, shuffle=True)\n",
        "valid_dataloader = DataLoader(valid, batch_size=32, shuffle=False)\n",
        "\n",
        "train_iter, valid_iter = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train_dataloader, valid_dataloader),\n",
        "    batch_sizes=(32,32),\n",
        "    sort_key=lambda x: len(x['text']),\n",
        "    repeat=True,\n",
        "    sort=False,\n",
        "    shuffle=True,\n",
        "    sort_within_batch=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "#text_field = torchtext.legacy.data.Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
        "#text_field.build_vocab(train, min_freq=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb6g58lIlUYO"
      },
      "source": [
        "text_tokenizer = word_tokenize\n",
        "label_tokenizer = lambda x: float(x)\n",
        "\n",
        "#label_field = torchtext.legacy.data.Field(sequential=False, dtype=torch.float, lower=False)\n",
        "#text_field = torchtext.legacy.data.Field(sequential=True, tokenize=text_tokenizer, use_vocab=False)\n",
        "\n",
        "label_field = torchtext.legacy.data.Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.long,tokenize=label_tokenizer)\n",
        "text_field = torchtext.legacy.data.Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
        "\n",
        "datafields = [('text', text_field),\n",
        "              ('label', label_field)]\n",
        "\n",
        "train, valid = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='',\n",
        "    train='train.csv',\n",
        "    validation='valid.csv',\n",
        "    format='CSV',\n",
        "    fields=datafields, \n",
        "    skip_header=True\n",
        ")\n",
        "\n",
        "train_iter, valid_iter = torchtext.legacy.data.BucketIterator.splits(\n",
        "    (train, valid),\n",
        "    batch_sizes=(32,32),\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    repeat=True,\n",
        "    sort=False,\n",
        "    shuffle=True,\n",
        "    sort_within_batch=True\n",
        "    device=device\n",
        ")\n",
        "\n",
        "text_field.build_vocab(train, min_freq=3)\n",
        "\n",
        "train_iter.create_batches()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQDm8fWod_9a"
      },
      "source": [
        "class DatasetBucket:\n",
        "  def __init__(self, data, embedding_vectorizer, label_binarizer, relation_to_class):\n",
        "    self.data = data\n",
        "    self.embedding_vectorizer = embedding_vectorizer\n",
        "    self.label_binarizer = label_binarizer\n",
        "    self.relation_to_class = relation_to_class\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data.index)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    data = self.data.iloc[index]\n",
        "    return self._item_vectorizing(data)\n",
        "\n",
        "  def _item_vectorizing(self, item):\n",
        "    utterance = item['utterances']\n",
        "    utterance_vec = self.embedding_vectorizer(utterance)\n",
        "\n",
        "    core_relation = item['Core Relations']\n",
        "    core_relation_vec = self._relation_to_values(core_relation)\n",
        "\n",
        "    return utterance_vec, core_relation_vec\n",
        "\n",
        "  def _relation_to_values(self, core_relation):\n",
        "    return self.relation_to_class[core_relation]\n",
        "\n",
        "  def _relation_to_vec(self, core_relation):\n",
        "    return self.label_binarizer.transform([core_relation])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5jNZWyd1zNe"
      },
      "source": [
        "destination_folder = './lstm/results'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPsErBUYwwXV"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, dimension=300):\n",
        "    super(LSTM, self).__init__()\n",
        "    \n",
        "    self.embedding = nn.Embedding(len(text_field.vocab), 300)\n",
        "    self.dimension=dimension\n",
        "    self.lstm = nn.LSTM(input_size=300,\n",
        "                        hidden_size=dimension,\n",
        "                        num_layers=1,\n",
        "                        batch_first=True,\n",
        "                        bidirectional=True)\n",
        "    \n",
        "    self.drop = nn.Dropout(p=0.5)\n",
        "    self.fc = nn.Linear(2*dimension, 47)\n",
        "\n",
        "  def forward(self, text, text_len):\n",
        "\n",
        "    text_emb = self.embedding(text)\n",
        "\n",
        "    packed_input = pack_padded_sequence(text_emb, text_len, batch_first=True, enforce_sorted=False)\n",
        "    packed_output, _ = self.lstm(packed_input)\n",
        "    output, _ = pad_packed_sequence(packed_output, batch_first = True)\n",
        "\n",
        "    out_forward = output[range(len(output)), text_len - 1, :self.dimension]\n",
        "    out_reverse = output[:, 0, self.dimension:]\n",
        "    out_reduced = torch.cat((out_forward, out_reverse), 1)\n",
        "    text_fea = self.drop(out_reduced)\n",
        "\n",
        "    text_fea = self.fc(text_fea)\n",
        "    #text_out = torch.squeeze(text_fea, 31)\n",
        "    #text_out = torch.sigmoid(text_fea)\n",
        "\n",
        "    return text_fea.transpose(-1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6pBfVG4M9B_"
      },
      "source": [
        "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer.state_dict(),\n",
        "                  'valid_loss': valid_loss}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_checkpoint(load_path, model, optimizer):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    model.load_state_dict(state_dict['model_state_dict'])\n",
        "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
        "    \n",
        "    return state_dict['valid_loss']\n",
        "\n",
        "\n",
        "def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):\n",
        "\n",
        "    if save_path == None:\n",
        "        return\n",
        "    \n",
        "    state_dict = {'train_loss_list': train_loss_list,\n",
        "                  'valid_loss_list': valid_loss_list,\n",
        "                  'global_steps_list': global_steps_list}\n",
        "    \n",
        "    torch.save(state_dict, save_path)\n",
        "    print(f'Model saved to ==> {save_path}')\n",
        "\n",
        "\n",
        "def load_metrics(load_path):\n",
        "\n",
        "    if load_path==None:\n",
        "        return\n",
        "    \n",
        "    state_dict = torch.load(load_path, map_location=device)\n",
        "    print(f'Model loaded from <== {load_path}')\n",
        "    \n",
        "    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgIWx2vuzQX5"
      },
      "source": [
        "def train(model,\n",
        "          optimizer,\n",
        "          criterion=cross_entropy,\n",
        "          train_loader=train_iter,\n",
        "          valid_loader=valid_iter,\n",
        "          num_epochs=5,\n",
        "          eval_every=len(train_iter),\n",
        "          file_path='./',\n",
        "          best_valid_loss=float('Inf')):\n",
        "  \n",
        "  running_loss = 0.0\n",
        "  valid_running_loss = 0.0\n",
        "  global_step = 0\n",
        "  train_loss_list = []\n",
        "  valid_loss_list = []\n",
        "  global_steps_list = []\n",
        "\n",
        "  train_loader.create_batches()\n",
        "\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "      text = batch.text[0]\n",
        "      label = batch.label\n",
        "\n",
        "      output = model(text, batch.text[1])\n",
        "\n",
        "      loss = criterion(output, label)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_every == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "          for batch in valid_loader:\n",
        "            text = batch.text[0]\n",
        "            label = batch.label\n",
        "\n",
        "            output = model(text, batch.text[1])\n",
        "\n",
        "            loss = criterion(output, label)\n",
        "            valid_running_loss += loss.item()\n",
        "        \n",
        "        average_train_loss = running_loss / eval_every\n",
        "        average_valid_loss = valid_running_loss / len(valid_loader)\n",
        "        train_loss_list.append(average_train_loss)\n",
        "        valid_loss_list.append(average_valid_loss)\n",
        "        global_steps_list.append(global_step)\n",
        "\n",
        "        running_loss = 0.0\n",
        "        valid_running_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n",
        "                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_loader),\n",
        "                              average_train_loss, average_valid_loss))\n",
        "        \n",
        "        if best_valid_loss > average_valid_loss:\n",
        "          best_valid_loss = average_valid_loss\n",
        "          save_checkpoint('model.pt', model, optimizer, best_valid_loss)\n",
        "          save_metrics('metrics.pt', train_loss_list, valid_loss_list, global_steps_list)\n",
        "    print('Finished Training!')\n",
        "\n",
        "model = LSTM().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "train(model=model, optimizer=optimizer, num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfR2-1cMp76e"
      },
      "source": [
        "def pad_collate_fn(batch):\n",
        "  things = [(torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
        "   for (seq_vectors, labels) in sorted(batch, key=lambda x: len(x[0]), reverse=True)]\n",
        "\n",
        "  utterance_vectors = [thing[0] for thing in things]\n",
        "  label_vectors = [thing[1] for thing in things]\n",
        "  lengths = [thing[2] for thing in things]\n",
        "\n",
        "  lengths = torch.LongTensor(lengths)\n",
        "  padded_utterance_vectors = pad_sequence(utterance_vectors, batch_first=True, padding_value=0)\n",
        "\n",
        "  label_vectors = torch.stack(list(label_vectors), dim=0)\n",
        "  return (padded_utterance_vectors, lengths), label_vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wM_JGlsKjCB"
      },
      "source": [
        "class RecurrentNet(nn.Module):\n",
        "  def __init__(self, lstm_network, fully_connected_network):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_states = None\n",
        "    self.lstm_network = lstm_network\n",
        "    self.fully_connected_network = fully_connected_network\n",
        "\n",
        "  def forward(self, padded_utterance_vectors, lengths):\n",
        "    total_length = padded_utterance_vectors.shape[1]\n",
        "\n",
        "    pack_padded_utterance_vectors = pack_padded_sequence(padded_utterance_vectors, lengths.cpu(), batch_first=True)\n",
        "    lstm_out, self.hidden_states = self.lstm_network(pack_padded_utterance_vectors)\n",
        "\n",
        "    lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True, total_length=total_length)\n",
        "\n",
        "    out_forward = lstm_out[range(len(lstm_out)), total_length - 1, :300]\n",
        "\n",
        "    tag_space = self.fully_connected_network(out_forward)\n",
        "\n",
        "    return tag_space.transpose(-1, 1)\n",
        "\n",
        "#full_network = RecurrentNet(lstm_network, fully_connected_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "uIWLCXt6nFkj",
        "outputId": "ff72a2ea-cb7f-4e23-a9d4-6348641f5755"
      },
      "source": [
        "pp = Preprocessor()\n",
        "data = pp.get_data(filename='train_data_merged_labels.csv', vectorize=False, class_column='Core Relations')\n",
        "new_data = pp.format_relations_data(data)\n",
        "relation_to_class = pp.classify_relation(new_data)\n",
        "train_data, dev_data = pp.split_data(new_data)\n",
        "\n",
        "vectorizer = Vectorizer(embedding_model)\n",
        "label_binarizer = LabelBinarizer()\n",
        "label_binarizer.fit(train_data['Core Relations'])\n",
        "train_dataset_vectorizer = DatasetBucket(train_data, vectorizer, label_binarizer=label_binarizer, relation_to_class=relation_to_class)\n",
        "valid_dataset_vectorizer = DatasetBucket(dev_data, vectorizer, label_binarizer=label_binarizer, relation_to_class=relation_to_class)\n",
        "\n",
        "utterance, label = train_dataset_vectorizer[0]\n",
        "\n",
        "dimension = 300\n",
        "num_layer = 1\n",
        "bidirectional = False\n",
        "\n",
        "lstm_network = nn.LSTM(input_size=dimension,\n",
        "                       hidden_size=dimension,\n",
        "                       num_layers=num_layer,\n",
        "                       bidirectional=bidirectional,\n",
        "                       batch_first=True)\n",
        "\n",
        "input_dim = dimension\n",
        "tag_dimension = 1\n",
        "\n",
        "fully_connected_network = nn.Linear(input_dim, tag_dimension)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "batch_size = 128\n",
        "lr = 0.1\n",
        "\n",
        "epoch_number = 10\n",
        "\n",
        "set_seeds(42)\n",
        "\n",
        "train_loader = DataLoader(train_dataset_vectorizer, batch_size=batch_size, shuffle=True, collate_fn=pad_collate_fn, num_workers=4)\n",
        "valid_loader = DataLoader(valid_dataset_vectorizer, batch_size=batch_size, collate_fn=pad_collate_fn, num_workers=4)\n",
        "\n",
        "full_network = RecurrentNet(lstm_network, fully_connected_network)\n",
        "\n",
        "optimizer = optim.SGD(full_network.parameters(), lr)\n",
        "\n",
        "exp = Experiment(\"./\", full_network, device=device, optimizer=optimizer,\n",
        "                 loss_function=cross_entropy, batch_metrics=[\"acc\"])\n",
        "\n",
        "exp.train(train_loader, valid_generator=valid_loader, epochs=epoch_number)\n",
        "#JUtils.get_dist_df(new_data['Core Relations'])\n",
        "#pp.original_class_fdist "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9dd3a1f07827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m exp = Experiment(\"./\", full_network, device=device, optimizer=optimizer,\n\u001b[0;32m---> 59\u001b[0;31m                  loss_function=cross_entropy, batch_metrics=[\"acc\"])\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/poutyne/framework/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, directory, network, device, logging, optimizer, loss_function, batch_metrics, epoch_metrics, monitoring, monitor_metric, monitor_mode, task)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         self.model = Model(\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         )\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, network, optimizer, loss_function, batch_metrics, epoch_metrics, device)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_network_optimizer_parameters_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_optim_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1716\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_loss_and_metrics_modules_to_right_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Resets _flat_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "C_TAu6SgtKq5",
        "outputId": "c6c19f09-8055-443f-e4a8-fda82703f679"
      },
      "source": [
        "pd.read_csv('train_data.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>utterances</th>\n",
              "      <th>IOB Slot tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>who plays luke on star wars new hope</td>\n",
              "      <td>O O B_char O B_movie I_movie I_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>show credits for the godfather</td>\n",
              "      <td>O O O B_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>who was the main actor in the exorcist</td>\n",
              "      <td>O O O O O O B_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>find the female actress from the movie she 's ...</td>\n",
              "      <td>O O O O O O O B_movie I_movie I_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>who played dory on finding nemo</td>\n",
              "      <td>O O B_char O B_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2307</th>\n",
              "      <td>2308</td>\n",
              "      <td>what was the revenue for toy story 3</td>\n",
              "      <td>O O O O O B_movie I_movie I_movie</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2308</th>\n",
              "      <td>2309</td>\n",
              "      <td>dark knight revenue</td>\n",
              "      <td>B_movie I_movie O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2309</th>\n",
              "      <td>2310</td>\n",
              "      <td>how much did the dark night generate</td>\n",
              "      <td>O O O B_movie I_movie I_movie O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2310</th>\n",
              "      <td>2311</td>\n",
              "      <td>can i see the lion king 's revenue</td>\n",
              "      <td>O O O B_movie I_movie I_movie O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2311</th>\n",
              "      <td>2312</td>\n",
              "      <td>can i see what the lion king 's revenue was</td>\n",
              "      <td>O O O O B_movie I_movie I_movie O O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2312 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID  ...                                  IOB Slot tags\n",
              "0        1  ...   O O B_char O B_movie I_movie I_movie I_movie\n",
              "1        2  ...                          O O O B_movie I_movie\n",
              "2        3  ...                    O O O O O O B_movie I_movie\n",
              "3        4  ...  O O O O O O O B_movie I_movie I_movie I_movie\n",
              "4        5  ...                   O O B_char O B_movie I_movie\n",
              "...    ...  ...                                            ...\n",
              "2307  2308  ...              O O O O O B_movie I_movie I_movie\n",
              "2308  2309  ...                              B_movie I_movie O\n",
              "2309  2310  ...                O O O B_movie I_movie I_movie O\n",
              "2310  2311  ...              O O O B_movie I_movie I_movie O O\n",
              "2311  2312  ...          O O O O B_movie I_movie I_movie O O O\n",
              "\n",
              "[2312 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}